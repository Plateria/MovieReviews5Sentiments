{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.datacamp.com/community/tutorials/text-analytics-beginners-nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Tokenization\n",
    "\n",
    "Sentence tokenizer breaks text paragraph into sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello Mr. Smith, how are you doing today?', 'The weather is great, and city is awesome.', 'The sky is pinkish-blue.', \"You shouldn't eat cardboard\"]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "text = \"\"\"Hello Mr. Smith, how are you doing today? The weather is great, and city is awesome.\n",
    "The sky is pinkish-blue. You shouldn't eat cardboard\"\"\"\n",
    "tokenized_sent=sent_tokenize(text)\n",
    "print(tokenized_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Tokenization\n",
    "\n",
    "Word tokenizer breaks text paragraph into words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'Mr.', 'Smith', ',', 'how', 'are', 'you', 'doing', 'today', '?', 'The', 'weather', 'is', 'great', ',', 'and', 'city', 'is', 'awesome', '.', 'The', 'sky', 'is', 'pinkish-blue', '.', 'You', 'should', \"n't\", 'eat', 'cardboard']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "tokenized_word=word_tokenize(text)\n",
    "print(tokenized_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frequency Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 25 samples and 30 outcomes>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('is', 3), (',', 2)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.probability import FreqDist\n",
    "fdist = FreqDist(tokenized_word)\n",
    "print(fdist)\n",
    "fdist.most_common(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fdist' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-11d2a5648fb1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Frequency Distribution Plot\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mfdist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcumulative\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'fdist' is not defined"
     ]
    }
   ],
   "source": [
    "# Frequency Distribution Plot\n",
    "import matplotlib.pyplot as plt\n",
    "fdist.plot(30,cumulative=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stopwords\n",
    "\n",
    "Stopwords considered as noise in the text. Text may contain stop words such as is, am, are, this, a, an, the, etc.\n",
    "\n",
    "In NLTK for removing stopwords, you need to create a list of stopwords and filter out your list of tokens from these words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'we', 'the', 'too', 'but', 'shouldn', 'do', \"doesn't\", 'how', 'haven', 'own', \"needn't\", \"she's\", 'm', \"you'd\", 'of', 'her', 'ours', 'then', 'by', 'most', \"weren't\", 'during', 'in', 'weren', 'me', 'won', 'am', \"wouldn't\", 'with', \"mightn't\", 't', 'into', 'only', 'just', 'needn', 'myself', 'are', 'under', \"mustn't\", 'couldn', 'hers', 'than', \"hadn't\", \"couldn't\", 'both', 'ourselves', 're', 'out', 'its', 'as', 'doesn', 'on', 'an', 'above', \"shan't\", 'himself', 'being', 'll', 'these', 'has', 'here', 'd', 'should', 'don', 'through', 'very', 'theirs', 'their', 'he', 'is', 'having', \"hasn't\", 'up', 'themselves', 'hasn', 'were', \"wasn't\", 'nor', \"won't\", 'his', 'had', \"you'll\", 'that', \"should've\", 'my', 'didn', 'hadn', 'mustn', 'this', 'you', 'if', 'where', 'each', 'or', 'before', 'o', 'ain', 'over', 'they', 'why', 'itself', 'y', 'down', 'be', 'now', 'mightn', 'those', 'yourselves', 'does', 'about', \"isn't\", 'same', 'below', 'been', 'until', 'at', 'have', 'there', \"haven't\", 'it', 'and', 'can', \"you've\", 'whom', 'while', 'did', 'more', 'herself', \"aren't\", 'some', 'between', 'him', \"didn't\", 'a', \"that'll\", 'shan', 'against', 'further', 'again', 'yourself', 'which', \"you're\", \"shouldn't\", 'wouldn', 'our', 'other', 'so', 'few', 've', 's', 'yours', 'them', 'from', 'doing', 'after', 'such', 'will', 'for', 'any', 'no', 'she', 'i', 'isn', 'not', 'because', 'all', 'what', 'was', \"don't\", 'off', 'aren', 'who', 'once', 'to', \"it's\", 'wasn', 'when', 'your', 'ma'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words=set(stopwords.words(\"english\"))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Sentence: ['Hello', 'Mr.', 'Smith', ',', 'how', 'are', 'you', 'doing', 'today', '?', 'The', 'weather', 'is', 'great', ',', 'and', 'city', 'is', 'awesome', '.', 'The', 'sky', 'is', 'pinkish-blue', '.', 'You', 'should', \"n't\", 'eat', 'cardboard']\n",
      "Filterd Sentence: ['Hello', 'Mr.', 'Smith', ',', 'today', '?', 'The', 'weather', 'great', ',', 'city', 'awesome', '.', 'The', 'sky', 'pinkish-blue', '.', 'You', \"n't\", 'eat', 'cardboard']\n"
     ]
    }
   ],
   "source": [
    "filtered_sent=[]\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        filtered_sent.append(w)\n",
    "print(\"Tokenized Sentence:\",tokenized_word)\n",
    "print(\"Filterd Sentence:\",filtered_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming\n",
    "\n",
    "Stemming is a process of linguistic normalization, which reduces words to their word root word or chops off the derivational affixes. For example, connection, connected, connecting word reduce to a common word \"connect\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Sentence: ['Hello', 'Mr.', 'Smith', ',', 'today', '?', 'The', 'weather', 'great', ',', 'city', 'awesome', '.', 'The', 'sky', 'pinkish-blue', '.', 'You', \"n't\", 'eat', 'cardboard']\n",
      "Stemmed Sentence: ['hello', 'mr.', 'smith', ',', 'today', '?', 'the', 'weather', 'great', ',', 'citi', 'awesom', '.', 'the', 'sky', 'pinkish-blu', '.', 'you', \"n't\", 'eat', 'cardboard']\n"
     ]
    }
   ],
   "source": [
    "# Stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "stemmed_words=[]\n",
    "for w in filtered_sent:\n",
    "    stemmed_words.append(ps.stem(w))\n",
    "\n",
    "print(\"Filtered Sentence:\",filtered_sent)\n",
    "print(\"Stemmed Sentence:\",stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization\n",
    "\n",
    "Lemmatization reduces words to their base word, which is linguistically correct lemmas. It transforms root word with the use of vocabulary and morphological analysis. Lemmatization is usually more sophisticated than stemming. Stemmer works on an individual word without knowledge of the context. For example, The word \"better\" has \"good\" as its lemma. This thing will miss by stemming because it requires a dictionary look-up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\marei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized Word: fly\n",
      "Stemmed Word: fli\n"
     ]
    }
   ],
   "source": [
    "#Lexicon Normalization\n",
    "#performing stemming and Lemmatization\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lem = WordNetLemmatizer()\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stem = PorterStemmer()\n",
    "\n",
    "word = \"flying\"\n",
    "print(\"Lemmatized Word:\",lem.lemmatize(word,\"v\"))\n",
    "print(\"Stemmed Word:\",stem.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS Tagging\n",
    "\n",
    "The primary target of Part-of-Speech(POS) tagging is to identify the grammatical group of a given word. Whether it is a NOUN, PRONOUN, ADJECTIVE, VERB, ADVERBS, etc. based on the context. POS Tagging looks for relationships within the sentence and assigns a corresponding tag to the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Albert', 'Einstein', 'was', 'born', 'in', 'Ulm', ',', 'Germany', 'in', '1879', '.']\n"
     ]
    }
   ],
   "source": [
    "sent = \"Albert Einstein was born in Ulm, Germany in 1879.\"\n",
    "tokens=nltk.word_tokenize(sent)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\marei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Albert', 'NNP'),\n",
       " ('Einstein', 'NNP'),\n",
       " ('was', 'VBD'),\n",
       " ('born', 'VBN'),\n",
       " ('in', 'IN'),\n",
       " ('Ulm', 'NNP'),\n",
       " (',', ','),\n",
       " ('Germany', 'NNP'),\n",
       " ('in', 'IN'),\n",
       " ('1879', 'CD'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.pos_tag(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performing Sentiment Analysis using Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>A series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                                             Phrase  \\\n",
       "0         1           1  A series of escapades demonstrating the adage ...   \n",
       "1         2           1  A series of escapades demonstrating the adage ...   \n",
       "2         3           1                                           A series   \n",
       "3         4           1                                                  A   \n",
       "4         5           1                                             series   \n",
       "\n",
       "   Sentiment  \n",
       "0          1  \n",
       "1          2  \n",
       "2          2  \n",
       "3          2  \n",
       "4          2  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('train.tsv', sep='\\t')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 156060 entries, 0 to 156059\n",
      "Data columns (total 4 columns):\n",
      "PhraseId      156060 non-null int64\n",
      "SentenceId    156060 non-null int64\n",
      "Phrase        156060 non-null object\n",
      "Sentiment     156060 non-null int64\n",
      "dtypes: int64(3), object(1)\n",
      "memory usage: 4.8+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    79582\n",
       "3    32927\n",
       "1    27273\n",
       "4     9206\n",
       "0     7072\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.Sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "Sentiment_count = data.groupby('Sentiment').count()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.bar(Sentiment_count.index.values, Sentiment_count['Phrase'])\n",
    "plt.xlabel('Review Sentiments')\n",
    "plt.ylabel('Number of Review')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can create a matrix of document and words by counting the occurrence of words in the given document. This matrix is known as Document-Term Matrix(DTM).\n",
    "\n",
    "This matrix is using a single word. It can be a combination of two or more words, which is called a bigram or trigram model and the general approach is called the n-gram model.\n",
    "\n",
    "You can generate document term matrix by using scikit-learn's CountVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "#tokenizer to remove unwanted elements from out data like symbols and numbers\n",
    "token = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
    "cv = CountVectorizer(lowercase=True, stop_words='english', ngram_range=(1,1), tokenizer=token.tokenize)\n",
    "text_counts=cv.fit_transform(data['Phrase'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split train and test set\n",
    "\n",
    "To understand model performance, dividing the dataset into a training set and a test set is a good strategy.\n",
    "\n",
    "Let's split dataset by using function train_test_split(). You need to pass basically 3 parameters features, target, and test_set size. Additionally, you can use random_state to select records randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    text_counts, data['Sentiment'], test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building and Evaluation\n",
    "\n",
    "Let's build the Text Classification Model using TF-IDF.\n",
    "\n",
    "First, import the MultinomialNB module and create a Multinomial Naive Bayes classifier object using MultinomialNB() function.\n",
    "\n",
    "Then, fit your model on a train set using fit() and perform prediction on the test set using predict()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB Accuracy: 0.6049169122986885\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "# Model Generation Using Multinomial Naive Bayes\n",
    "clf = MultinomialNB().fit(X_train, y_train)\n",
    "predicted= clf.predict(X_test)\n",
    "print(\"MultinomialNB Accuracy:\",metrics.accuracy_score(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, you got a classification rate of 60.49% using CountVector(or BoW), which is not considered as good accuracy. We need to improve this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Generation using TF-IDF\n",
    "\n",
    "In Term Frequency(TF), you just count the number of words occurred in each document. The main issue with this Term Frequency is that it will give more weight to longer documents. Term frequency is basically the output of the BoW model.\n",
    "\n",
    "IDF(Inverse Document Frequency) measures the amount of information a given word provides across the document. IDF is the logarithmically scaled inverse ratio of the number of documents that contain the word and the total number of documents. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF(Term Frequency-Inverse Document Frequency) normalizes the document term matrix. It is the product of TF and IDF. Word with high tf-idf in a document, it is most of the times occurred in given documents and must be absent in the other documents. So the words must be a signature word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tf=TfidfVectorizer()\n",
    "text_tf= tf.fit_transform(data['Phrase'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split train and test set (TF-IDF)\n",
    "\n",
    "Let's split dataset by using function train_test_split(). You need to pass basically 3 parameters features, target, and test_set size. Additionally, you can use random_state to select records randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    text_tf, data['Sentiment'], test_size=0.3, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building and Evaluation (TF-IDF)\n",
    "\n",
    "Let's build the Text Classification Model using TF-IDF.\n",
    "\n",
    "First, import the MultinomialNB module and create the Multinomial Naive Bayes classifier object using MultinomialNB() function.\n",
    "\n",
    "Then, fit your model on a train set using fit() and perform prediction on the test set using predict()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB Accuracy: 0.5865265496176684\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "# Model Generation Using Multinomial Naive Bayes\n",
    "clf = MultinomialNB().fit(X_train, y_train)\n",
    "predicted= clf.predict(X_test)\n",
    "print(\"MultinomialNB Accuracy:\",metrics.accuracy_score(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, you got a classification rate of 58.65% using TF-IDF features, which is not considered as good accuracy. We need to improve the accuracy by using some other preprocessing or feature engineering. Let's suggest in comment box some approach for accuracy improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
